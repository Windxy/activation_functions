# activation-function

激活函数（Activation Function）是神经网络中的一个关键组件，它决定了一个神经元的输出。激活函数的主要作用是引入非线性，使得神经网络能够处理复杂的任务，如图像识别、自然语言处理等。以下是一些常见的激活函数及其特点：

| 激活函数                    | 时间 | 定义                                                         | 特点                                                         | 缺点                                                         |
| --------------------------- | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Sigmoid**                 | -    | $\sigma(x) = \frac{1}{1 + e^{-x}}$                      | 输出范围在 (0, 1) 之间，适用于二分类问题。                   | 容易导致梯度消失问题。                                       |
| **Tanh**                    | -    | $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$           | 输出范围在 (-1, 1) 之间，相对于Sigmoid函数，Tanh函数的梯度较大。 | 同样可能会导致梯度消失问题。                                 |
| **ReLU**                    | 2010 | $\text{ReLU}(x) = \max(0, x)$                            | 计算简单，不会饱和（不会出现梯度消失问题），广泛应用于深层神经网络。 | 当输入为负值时，梯度为零，可能导致“神经元死亡”问题。         |
| **Leaky ReLU**              | 2014 | $\text{Leaky ReLU}(x) = \max(\alpha x, x)$ ，其中 $\alpha$ 是一个小常数。 | 解决了ReLU的“神经元死亡”问题，因为负值输入也有一个很小的梯度。 | 非对称的结构可能会让模型偏向于正向输出，且同样存在梯度消失问题 |
| **Parametric ReLU (PReLU)** | 2015 | $\text{PReLU}(x) = \max(\alpha x, x) $，其中 $\alpha $是一个可学习参数。 | 通过学习参数 $\alpha $$，可以更好地适应数据。              | 更多的自由度引入了较高的灵活度，但也可能引入过拟合风险；α作为可学习参数，可能进入一个局部最优解 |
| **SiLU**                    | 2017 | $\text{SiLU}(x) = x \cdot \text{Sigmoid}(x)$           | 平滑性、非单调性、实验效果好。                               | 计算较为复杂。                                               |
| **ELU**                     | 2015 | $\text{ELU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha (e^x - 1) & \text{if } x \leq 0\end{cases}$| 在负区间有非零渐变，可以缓解ReLU的“神经元死亡”问题，并且具有更好的鲁棒性。 | -                                                            |
| **GELU**                    | 2016 | $GELU=x\cdot\Phi(x)$，其中$$\Phi(x)=\frac{1}{2}\left[1+\text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]$$，标准正态分布的累积分布函数（CDF），erf是损失函数，近似函数为$\text{GELU}(x) = 0.5 x (1 + \tanh [\sqrt{2/\pi} (x + 0.044715 x^3)])$| 平滑性、概率解释性、无死区问题                               | 计算复杂度高                                                 |
| **Softmax**                 | -    | $Softmax(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$      | 通常用于多分类问题，将输入向量转化为概率分布。               | 计算复杂、数据溢出或下溢（涉及指数运算）、类别不平衡（小数据会使之梯度较小）、类别独立性假设、过拟合风险（输出是概率分布） |
| **SwiGLU**                  | 2020 | $\text{SwiGLU}(x) = \text{Swish}(x) \odot (\mathbf{W}_1 x + b_1)$ <br/> 其中，$\text{Swish}(x)$ 是 Swish 激活函数，定义为：<br/>$\text{Swish}(x) = x \cdot \sigma(x)$<br/>而 $\odot$ 表示元素级别的乘法操作，$\mathbf{W}_1$ 和 $b_1$ 分别是权重矩阵和偏置项。 | 平滑性、门控机制、非线性表达能力                             | 复杂度高                                                     |

- 较弱的激活函数可能会导致梯度的问题，但较复杂的结构也可能引入过拟合的风险
